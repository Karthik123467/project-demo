import os
import streamlit as st
import pickle
from dotenv import load_dotenv
from langchain.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.document_loaders import (
    UnstructuredURLLoader,
    TextLoader,
    PyPDFLoader
)
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS

# Load API key from Streamlit secrets or .env (for local testing)
if "OPENAI_API_KEY" in st.secrets:
    os.environ["OPENAI_API_KEY"] = st.secrets["OPENAI_API_KEY"]
else:
    load_dotenv()

st.set_page_config(page_title="EduBot: Academic Research Assistant ğŸ“š", page_icon="ğŸ“š", layout="wide")

st.markdown("""
    <style>
    .stApp {background: linear-gradient(to right, #141e30, #243b55); color: #ffffff;}
    .title {font-size: 2.8em; font-weight: bold; text-align: center; color: #00c6ff;}
    .subtitle {font-size: 1.2em; text-align: center; color: #eeeeee;}
    .stButton > button {background-color: #00c6ff; color: #000; border-radius: 8px; padding: 0.6em 1.4em; font-weight: bold;}
    .stButton > button:hover {background-color: #7df9ff;}
    .footer {text-align: center; color: #bbbbbb; margin-top: 30px;}
    </style>
""", unsafe_allow_html=True)

st.markdown('<div class="title">EduBot: Academic Research Assistant ğŸ“š</div>', unsafe_allow_html=True)
st.markdown('<div class="subtitle">Summarize & Query Research Papers Instantly</div>', unsafe_allow_html=True)

# Sidebar Inputs
with st.sidebar:
    st.subheader("ğŸ“‘ Enter Research Paper URLs (Static Pages Only!)")
    urls = [st.text_input(f"ğŸ”— Paper URL {i+1}") for i in range(3)]
    process_url_clicked = st.button("ğŸš€ Process Papers from URLs")
    
    st.markdown("---")
    st.subheader("ğŸ“‚ Or Upload PDF/Text Files")
    uploaded_files = st.file_uploader("Upload Files", type=["pdf", "txt"], accept_multiple_files=True)
    process_file_clicked = st.button("ğŸš€ Process Uploaded Files")

file_path = "edu_faiss_store.pkl"
progress_placeholder = st.empty()
llm = ChatOpenAI(temperature=0.2, max_tokens=1500, model_name="gpt-3.5-turbo")

# Process URLs
if process_url_clicked:
    with st.spinner("Processing URLs..."):
        loader = UnstructuredURLLoader(urls=urls)
        data = loader.load()
        if not data or all(not d.page_content.strip() for d in data):
            st.error("âŒ No valid text found from the URLs. Blogs and dynamic sites may not work. Try Wikipedia, arXiv, etc., or upload files instead.")
            st.stop()
        
        progress_placeholder.progress(33, "Loaded Papers âœ…")
        text_splitter = RecursiveCharacterTextSplitter(separators=['\n\n', '\n', '.', ','], chunk_size=3000)
        docs = text_splitter.split_documents(data)
        if not docs:
            st.error("âŒ No text chunks were created from the papers.")
            st.stop()

        progress_placeholder.progress(66, "Split Text âœ…")
        embeddings = OpenAIEmbeddings()
        vectorstore = FAISS.from_documents(docs, embeddings)
        progress_placeholder.progress(100, "Vectorstore Built âœ…")

        with open(file_path, "wb") as f:
            pickle.dump(vectorstore, f)

        st.success("âœ… Papers Processed Successfully from URLs!")

# Process Uploaded Files
if process_file_clicked:
    if not uploaded_files:
        st.error("âŒ Please upload at least one file.")
        st.stop()

    all_docs = []
    with st.spinner("Processing uploaded files..."):
        for uploaded_file in uploaded_files:
            file_name = uploaded_file.name
            if file_name.endswith(".txt"):
                text = uploaded_file.read().decode("utf-8")
                docs = [Document(page_content=text, metadata={"source": file_name})]
            elif file_name.endswith(".pdf"):
                with open(file_name, "wb") as f:
                    f.write(uploaded_file.read())
                loader = PyPDFLoader(file_name)
                docs = loader.load()
                os.remove(file_name)
            else:
                continue
            all_docs.extend(docs)

        if not all_docs:
            st.error("âŒ Failed to process any uploaded files.")
            st.stop()

        progress_placeholder.progress(33, "Loaded Files âœ…")
        text_splitter = RecursiveCharacterTextSplitter(separators=['\n\n', '\n', '.', ','], chunk_size=3000)
        docs = text_splitter.split_documents(all_docs)
        if not docs:
            st.error("âŒ No text chunks were created from the uploaded files.")
            st.stop()

        progress_placeholder.progress(66, "Split Text âœ…")
        embeddings = OpenAIEmbeddings()
        vectorstore = FAISS.from_documents(docs, embeddings)
        progress_placeholder.progress(100, "Vectorstore Built âœ…")

        with open(file_path, "wb") as f:
            pickle.dump(vectorstore, f)

        st.success("âœ… Files Processed Successfully!")

# Query Section
query = st.text_input("ğŸ’¬ Ask Your Academic Question:")
if query:
    if os.path.exists(file_path):
        with open(file_path, "rb") as f:
            vectorstore = pickle.load(f)

        chain = RetrievalQA.from_chain_type(
            llm=llm, retriever=vectorstore.as_retriever(search_kwargs={"k": 5}), return_source_documents=True
        )

        with st.spinner("Generating Answer..."):
            result = chain.invoke({"query": query})

        st.header("ğŸ” Answer")
        st.success(result["result"])

        source_docs = result.get("source_documents", [])
        unique_sources = set()
        for doc in source_docs:
            source = doc.metadata.get("source", "")
            if source:
                unique_sources.add(source)

        if unique_sources:
            with st.expander("ğŸ“‚ Sources Used"):
                for src in unique_sources:
                    st.write(src)
        else:
            st.info("No sources found for this answer.")

# Footer
st.markdown('<div class="footer">ğŸš€ Created by PKK</div>', unsafe_allow_html=True)
